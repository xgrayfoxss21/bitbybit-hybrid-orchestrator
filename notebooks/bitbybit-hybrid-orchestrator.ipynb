{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjprqBDphHEwcEOdXKUNxb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xgrayfoxss21/bitbybit-hybrid-orchestrator/blob/main/notebooks/bitbybit-hybrid-orchestrator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title BitNet-7B PoC ‚Äî Colab Bootstrap\n",
        "# @markdown Fill your settings then run the cell.\n",
        "\n",
        "# ==== Repo ====\n",
        "REPO_URL = \"https://github.com/xgrayfoxss21/BitNet-7B-PoC-KD-Distillation-Mini-Training-7B-Dry-Run\"\n",
        "REPO_DIR = \"/content/bitnet-7b-poc\"  # folder to clone into\n",
        "\n",
        "# ==== Google Drive ====\n",
        "AUTO_MOUNT_GDRIVE = True  #@param {type:\"boolean\"}\n",
        "GDRIVE_MOUNT_POINT = \"/content/drive\"  #@param {type:\"string\"}\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/bitnet_poc\"  #@param {type:\"string\"}\n",
        "\n",
        "# ==== Provider selection ====\n",
        "# one of: openai | anthropic | groq | aimlapi | gemini\n",
        "PROVIDER = \"openai\"  #@param [\"openai\",\"anthropic\",\"groq\",\"aimlapi\",\"gemini\"]\n",
        "\n",
        "# ==== API keys (paste only what you use) ====\n",
        "OPENAI_API_KEY   = \"\"  #@param {type:\"string\"}\n",
        "ANTHROPIC_API_KEY= \"\"  #@param {type:\"string\"}\n",
        "GROQ_API_KEY     = \"\"  #@param {type:\"string\"}\n",
        "AIMLAPI_API_KEY  = \"\"  #@param {type:\"string\"}\n",
        "GEMINI_API_KEY   = \"\"  #@param {type:\"string\"}\n",
        "\n",
        "# Optional OpenAI-compatible base URLs (leave blank if unsure)\n",
        "OPENAI_BASE_URL  = \"\"  #@param {type:\"string\"}\n",
        "GROQ_BASE_URL    = \"https://api.groq.com/openai/v1\"  # good default\n",
        "AIMLAPI_BASE_URL = \"https://api.aimlapi.com/v1\"      # good default\n",
        "\n",
        "# ==== Training/runtime toggles ====\n",
        "TORCH_DTYPE = \"bf16\"  #@param [\"bf16\",\"fp16\",\"fp32\"]\n",
        "ENABLE_FLASH_ATTN = 1 #@param {type:\"number\"}\n",
        "WANDB_DISABLED = 1    #@param {type:\"number\"}\n",
        "\n",
        "# Tokenizer / template (keep defaults unless you know you need to change)\n",
        "TOKENIZER_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  #@param {type:\"string\"}\n",
        "TEMPLATE = \"<|user|>\\n{prompt}\\n\\n<|assistant|>\\n\"     #@param {type:\"string\"}\n",
        "\n",
        "# ---- Bootstrapping logic (no edits below) ----\n",
        "import os, sys, textwrap, subprocess, json, shutil, pathlib\n",
        "\n",
        "def sh(cmd, check=True):\n",
        "    print(f\"$ {cmd}\")\n",
        "    r = subprocess.run(cmd, shell=True, check=check, text=True)\n",
        "    return r\n",
        "\n",
        "# 0) GPU info (nice to see)\n",
        "try:\n",
        "    sh(\"nvidia-smi\", check=False)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# 1) Clone / Pull repo\n",
        "if os.path.exists(REPO_DIR):\n",
        "    print(\"Repo dir exists ‚Äî pulling latest...\")\n",
        "    os.chdir(REPO_DIR)\n",
        "    sh(\"git pull --ff-only\")\n",
        "else:\n",
        "    os.chdir(\"/content\")\n",
        "    sh(f\"git clone {REPO_URL} {REPO_DIR}\")\n",
        "    os.chdir(REPO_DIR)\n",
        "\n",
        "# 2) Install Python deps\n",
        "sh(\"python -m pip -q install --upgrade pip\")\n",
        "sh(\"pip -q install -r requirements.txt\")\n",
        "\n",
        "# 3) Optionally mount Drive now (storage.py can also mount automatically)\n",
        "if AUTO_MOUNT_GDRIVE:\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount(GDRIVE_MOUNT_POINT)\n",
        "    except Exception as e:\n",
        "        print(\"Drive mount skipped or failed:\", e)\n",
        "\n",
        "# 4) Write .env from the form values (overwrites existing .env in this runtime)\n",
        "env = f\"\"\"\n",
        "# ==== STORAGE / PATHS ====\n",
        "AUTO_MOUNT_GDRIVE={1 if AUTO_MOUNT_GDRIVE else 0}\n",
        "GDRIVE_MOUNT_POINT={GDRIVE_MOUNT_POINT}\n",
        "DRIVE_ROOT={DRIVE_ROOT}\n",
        "CHECKPOINTS_DIR={DRIVE_ROOT}/checkpoints\n",
        "DATA_DIR={DRIVE_ROOT}/data\n",
        "REPORTS_DIR={DRIVE_ROOT}/reports\n",
        "LOGS_DIR={DRIVE_ROOT}/logs\n",
        "HF_HOME={DRIVE_ROOT}/.hf\n",
        "TRANSFORMERS_CACHE={{HF_HOME}}/transformers\n",
        "HF_DATASETS_CACHE={{HF_HOME}}/datasets\n",
        "TORCH_HOME={DRIVE_ROOT}/.torch\n",
        "TOKENIZERS_PARALLELISM=false\n",
        "PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128\n",
        "\n",
        "# ==== PROVIDERS ====\n",
        "PROVIDER={PROVIDER}\n",
        "OPENAI_API_KEY={OPENAI_API_KEY}\n",
        "ANTHROPIC_API_KEY={ANTHROPIC_API_KEY}\n",
        "GROQ_API_KEY={GROQ_API_KEY}\n",
        "AIMLAPI_API_KEY={AIMLAPI_API_KEY}\n",
        "GEMINI_API_KEY={GEMINI_API_KEY}\n",
        "OPENAI_BASE_URL={OPENAI_BASE_URL}\n",
        "GROQ_BASE_URL={GROQ_BASE_URL}\n",
        "AIMLAPI_BASE_URL={AIMLAPI_BASE_URL}\n",
        "OPENAI_MODEL=gpt-4o-mini\n",
        "ANTHROPIC_MODEL=claude-3-5-sonnet-20241022\n",
        "GROQ_MODEL=llama-3.1-70b-versatile\n",
        "AIMLAPI_MODEL=gpt-4o-mini\n",
        "GEMINI_MODEL=gemini-1.5-pro\n",
        "\n",
        "# ==== TRAINING ====\n",
        "TORCH_DTYPE={TORCH_DTYPE}\n",
        "ENABLE_FLASH_ATTN={ENABLE_FLASH_ATTN}\n",
        "WANDB_DISABLED={WANDB_DISABLED}\n",
        "WANDB_PROJECT=bitnet-poc\n",
        "WANDB_ENTITY=\n",
        "CUDA_VISIBLE_DEVICES=\n",
        "TOKENIZER_NAME={TOKENIZER_NAME}\n",
        "TEMPLATE={TEMPLATE}\n",
        "HF_TOKEN=\n",
        "\"\"\".strip() + \"\\n\"\n",
        "\n",
        "with open(\".env\", \"w\") as f:\n",
        "    f.write(env)\n",
        "print(\"üìù Wrote .env\")\n",
        "\n",
        "# 5) Prepare storage (mounts if needed, creates folders)\n",
        "sh(\"python -m scripts.storage\")\n",
        "\n",
        "print(\"\\n‚úÖ Bootstrap complete.\")\n",
        "print(\"Next steps:\")\n",
        "print(\"  ‚Ä¢ Run a target, e.g.:  !make teacher   or   !make collect\")\n"
      ],
      "metadata": {
        "id": "HRq7-gNhv_cN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}